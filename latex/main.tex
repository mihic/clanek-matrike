\documentclass[a4paper,11pt]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[textwidth=16cm,textheight=24cm]{geometry}

\usepackage{lmodern}
\renewcommand*\familydefault{\sfdefault}
\usepackage{textcomp} 
     
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{hyperref}

%% LASTNI PAKETI, DEFINICIJE
\newcommand{\cpp}{C\texttt{++} }
%% LASTNI PAKETI, DEFINICIJE KONEC


\title{Matrix Multiplication: Practical Use of a Strassen Like Algorithm}
\author{Rozman, Mitja and Eleršič, Miha}
\date{}

% # very important concept
% \def\vic#1{\emph{#1}}
% # identifier
% \def\id#1{\textsf{#1}}
% # quotes
% \def\q#1{"\textit{#1}"}

\sloppy


% % % % % % % % % % % Try to match IPSI style
\usepackage{lettrine}
\usepackage{setspace}

\usepackage{balance}
\setlength{\columnsep}{1cm}
\twocolumn

\pagenumbering{gobble}

% title
\makeatletter
\def\@maketitle{
\newpage
\let \footnote \thanks
\begin{center}
 	{\@setfontsize\Huge{26}{30}\@title\par}
	\vskip 2em
	{\large \@author \par}
\end{center}
\vskip 2em \par
}

% abstract
\renewenvironment{abstract}
{\begin{spacing}{1}\small\textbf{Abstract:}\bfseries\itshape}
{\end{spacing}}

%\usepackage[sc]{titlesec}
\usepackage{titlesec}
\titleformat{\section}[hang]{\normalfont\scshape\centering}{\sf\thesection.}{1em}{}
\titleformat{\subsection}[hang]{\normalfont}{\sf\thesubsection}{1em}{}

\usepackage{natbib}
\def\bibfont{\footnotesize}
\setlength{\bibsep}{0.0pt}
% % % % % % % % % % %


\begin{document}
\maketitle



\begingroup
\renewcommand\thefootnote{}
\footnotetext{Manuscript received Nov, 2017.}
\footnotetext{The authors are with the Faculty of Computer and Information Science, University of Ljubljana, Slovenia (e-mail: mitja.rozman@student.fmf.uni-lj.si).}
\endgroup

\begin{abstract}
In this paper we present and experimentally evaluate the performance of several practical algorithms for the matrix multiplication problem. We explain and implement classical algorithms, recursive algorithms as well as the Strassen algorithm. Additionally, we also present and evaluate several code tuning techniques. Our comparison includes our implementations of the algorithms as well as the algorithms from the uBLAS standard library. Finally, we also examine numerical stability of the algorithms.
\end{abstract}
\vspace{0.5cm}
\begin{spacing}{0.9}
\small\textbf{Index Terms}:  \textbf{\textit {matrix multiplication, algorithm, Strassen, experiment, performance}}
\end{spacing}

%\tableofcontents

\section{Introduction}

%%TODO through article we will refer to the $n \times n$ square matrix as a matrix of size $n$
%% TODO: mogoce bi pa pisali A=BC, potem lahko oznacimo D=tr(C)
%% trenutni problem je da X ne pase notri
%% se vedno pa ne bi morali ponazoriti sistema z le enim indeksom
%% res pa je da je matrika X bistveno drugacna od matrike C,
%% to je namrec matrika, ki je v spominu shranjena ravno obratno (ne row major)
%% mogoce so zato vseeno razogi za vizualno razlocevanje

\lettrine{M}{atrix} multiplication 
%problem
%%(TODO jaz nasprotujem uporabi problem na tem mestu, saj je potem one of the basic operations.. pa se mi zdi, da se to med seboj tepe)
is one of the basic operations used in many different fields such 
as machine learning, engineering and physics simulations.
%% TODO bolj konkretno
This makes implementing efficient and numerically stable algorithms for the operation important. 
In this article we compare different methods of matrix multiplication and their implementations. 
%% TODO ponovi se ideja 
The main goal is to compare execution time of matrix multiplication algorithms when the size of the matrix grows.
%%(TODO prej je bilo: on large matrices.)
%%on large matrices.
%%We implemented 4 different algorithms, from the basic *from definition) to a subcubic time complexity

%%\subsection{Known algorithms overview}

Matrix multiplication is a binary operation on two matrices 
$A$ and $B$ resulting in matrix $C=A \cdot B$. Each entry in the new matrix is defined as scalar product of the corresponding row in the first matrix and the corresponding column in the second, i.e.,
\begin{align*}
c_{i,j} = \sum_{k=1}^{n} a_{i,k} b_{k,j}.
\end{align*}
For matrix $A$ of size $m \times n$ and matrix $B$ of size $n \times k$ we can therefore compute matrix C of size $m \times k$. %$C= A \cdot B$ 


Most known matrix multiplication algorithms are derived directly from this definition and only change the order of addition operations. 
%% in the given ring. 
%%Examples of this kind of algorithms are classical iterative algorithm (which most faithfully obeys this rule), block algorithm and recursive algorithm.
%%TODO Recursive algorithms have potential for distributed computing.
%%TODO Block algorithms are usually most optimized for cache.
Examples of this kind of algorithms are classic and recursive algorithm as well as block matrix multiplication.
They all have asymptotic time complexity $\Theta(n^3)$ for two square $n \times n$ matrices.

There also exist algorithms with sub-cubic time complexity. These algorithms are of great theoretical importance but are usually not implemented.
Authors who worked on this field are Strassen \cite{Strassen1969}, Pan \cite{Pan1978a}, Bini \cite{Bini1979}, Schonhage \cite{Schoenhage1971}, Romani \cite{Romani1982}, Coppersmith \cite{Coppersmith1982}, Winograd \cite{Coppersmith1982}, Stothers \cite{Davie2013}, Williams \cite{Williams}.

Unfortunately most of the sub-cubic algorithms are very elaborate 
%%to implement 
and sometimes come with a much larger constant, meaning that even in theory they would not be faster on practically sized matrices.
This article tries to promote a Strassen-like algorithm, which is
straightforward to understand and fairly simple to implement. 
We hope to demonstrate its practical value.

%%\subsection{Existing implementations}

%%TODO nekje povedati katere zastavice je potrebno dodati, kaj je treba dati za include
For comparison evaluation of our results we used uBLAS from the \cpp library boost \cite{boost}. We used Boost because of its popularity and ease of use.
%%http://scicomp.stackexchange.com/questions/351/recommendations-for-a-usable-fast-c-matrix-library
%%https://stackoverflow.com/questions/11066925/good-matrix-libraries
Possible alternative libraries are Armadilo \cite{armadillo}, Eigen \cite{eigen} and Intel Math Kernel Library \cite{mkl}.

During the course of development of the algorithms presented in this paper we followed the systematic approach to generation of new ideas in research in computing \cite{blagojevic}. Additionally, we also followed the process and methods of the algorithm engineering approach to development of practically efficient algorithms \cite{Muller10} \cite{McGeoch12} \cite{ExpAlgDF}.

In the next section we describe the classical matrix multiplication method. In Section \ref{section:recursive} we describe the divide and conquer approach to matrix multiplication. In section \ref{section:strassen} we describe the Strassen method of matrix multiplication along with our own method similar to the Strassen method. In Section \ref{section:num_stab} we compare the numerical stability of the implemented algorithms. Finally, in Section \ref{section:experim} we compare the performance of our implemented algorithms.

\section{Classical Algorithm}
\label{section:classic}
The most basic algorithm for matrix multiplication is derived directly from the definition of matrix multiplication using three for loops.
In this basic algorithm we iterate thorough the rows of the first matrix and multiply each row with every column of the second matrix. For every row and column we compute the sum of the products of corresponding row and column entires.

\subsection{Transposed Matrix}
\label{classic_transposed}
The standard way of representing dense matrices in computer memory are two dimensional arrays. Since the 2D array has to be stored in memory which is one dimensional, it needs to be linearized. This can be done in a row-major order or column-major order \cite{Knuth1997}. The ordering has a significant effect on the algorithm performance, because accessing consecutive elements in memory is faster on modern CPUs because of caching. However classic multiplication accesses one matrix consecutively by rows and the other consecutively by columns. Hence, regardless of the order we choose to store the matrices in, one of the matrices is accessed inefficiently.
%%TODO kaj takega notri ja ali ne?
%%We can write X = tr(B) and multiplication vith transposed matrix as C = AB = A \times X
%%here X is transposed matrix of B and \times is product of..
%%Jaz bi rad dal kaj takega ampak je tezko nekam smiselno umestiti, bi pa rad to povedal nekje

Fortunately this can be fixed by transposing one of the matrices before multiplication. The question is, whether the overhead of first transposing one of the matrices is larger than the potential gains of better memory access pattern during multiplication.

In Figure \ref{fig:classic} we compare the performance of Classic algorithm and a version where the second matrix is transposed.
From the Figure we observe that the transposed version is faster. We can conclude that the overhead of transposing the matrix is lower than the speedup resulting from better use of the processor cache.

\begin{figure}[h]
\centering
\includegraphics[width=0.48\textwidth]{img/classic.pdf}
\caption{Running-time comparison of classic multiplication algorithms: without (\textsf{classic}) and with (\textsf{classicT}) transposition.}
\label{fig:classic}
\end{figure}

We can also observe higher variation in the running times of the non-transposed version. Although the results may look noisy, running more tests revealed the variation is consistent. In Figure \ref{fig:classic_var} we can see the results of running $10$ tests for $10$ consecutive values of $n$. We can see that for each $n$ we get consistent results, but changing $n$ by one can change the results significantly.  
%%TODO: dodamo se classic_tranposed na Figure 2 + bluzenje o tem kako predictor zna al pa ne zna.

\begin{figure}[h]
\centering
\includegraphics[width=0.48\textwidth]{img/classic_variation.pdf}

\caption{Running-time of untransposed classic algorithm on consecutive $n$. Each cross represents one test run without averaging.}
\label{fig:classic_var}
\end{figure}

\subsection{Implementation with Vectors}

For comparison we also implemented the classic algorithms with a nested vector implementation in \cpp (\verb$vector<vector<double>>$). This implementation allocates each row of the matrix separately, and the outer vector stores only references to the rows. In Figure \ref{fig:classic_old} we can see the same comparison as in Figure \ref{fig:classic}, but using the nested vector data structure. Again the transposed version is faster, but this time the difference is more significant. If we compare the nested vector implementations to the linearized row-major implementations we observe that the untransposed classic multiplication is slower, but the transposed version is not.
\begin{figure}[h]
\centering
\includegraphics[width=0.48\textwidth]{img/classic_old.pdf}
\caption{Running-time comparison of classic multiplication algorithms using a nested vector implementation: without (\textsf{vec\_classic}) and with (\textsf{vec\_classicT}) transposition. }
\label{fig:classic_old}
\end{figure}



\section{Recursive Algorithm}
\label{section:recursive}

Matrix multiplication can also be implemented with a divide and conquer approach\cite{Cormen2009}. We divide the matrices from the multiplication $C=AB$ into four blocks
%%
\begin{align*}
\begin{bmatrix}
C_{1,1} & C_{1,2} \\
C_{2,1} & C_{2,2}
\end{bmatrix}
=
\begin{bmatrix}
A_{1,1} & A_{1,2} \\
A_{2,1} & A_{2,2}
\end{bmatrix}
\begin{bmatrix}
B_{1,1} & B_{1,2} \\
B_{2,1} & B_{2,2}
\end{bmatrix}
_{\text{.}}
\end{align*}
%
Each block can be computed as follows.
\begin{align*}
C_{1,1} &= A_{1,1}B_{1,1} + A_{1,2}B_{2,1} \\
C_{1,2} &= A_{1,1}B_{1,2} + A_{1,2}B_{2,2} \\
C_{2,1} &= A_{2,1}B_{1,1} + A_{2,2}B_{2,1} \\
C_{2,2} &= A_{1,2}B_{1,2} + A_{2,2}B_{2,2}
\end{align*}
%% TODO tu velja omeniti, da se produkti blocnih podmatrik spet racunajo z istim algoritmom do dolocene meje (to je pa ze)
%% TODO blocni algoritem tu se ni razlozen ali je korektno da se ga omeni na tak nacin?

Each recursive step reduces the problem size and eventually the matrices are small enough that additional recursive steps are no longer optimal and it is more efficient to use the classic algorithm for such matrices. The minimal size of the matrix for which additional recursive steps are taken is analogous to the block size in the block algorithm. Additionally, such scheme also enables straightforward parallelization since the subproblems can be computed by multiple threads.

Since we use the classic algorithm in the final stages of recursion, we can also try transposing the matrix first as in Section \ref{classic_transposed}. 
This can be also written in the form of $C = A \times X$ where $X$ represents transposed matrix $B$ and $\times$ corresponding transposed product. 
At this point we write block partition in different style to avoid confusion with untransposed version of algorithm. 
So we can partition matrices $A$ and $X$ on blocks as
\begin{align*}
A = 
\begin{bmatrix}
A_{1} & A_{2} \\
B_{1} & B_{2}
\end{bmatrix},
\quad
X = 
\begin{bmatrix}
X_{1} & X_{2} \\
Y_{1} & Y_{2}
\end{bmatrix}
.
\end{align*}
And the resulting matrix $C$ can be computed as
\begin{align*}
C_{1,1} &= A_{1} \times X_{1} + A_{2}  \times X_{2} \\
C_{1,2} &= A_{1} \times Y_{1} + A_{2}  \times Y_{2} \\
C_{2,1} &= B_{1} \times X_{1} + B_{2}  \times X_{2} \\
C_{2,2} &= B_{1} \times Y_{1} + B_{2}  \times Y_{2}.
\end{align*}
In Figure \ref{fig:recursive} we compare these two versions. The blocks now fit in the cache and the gains from memory access patterns withing blocks are not as significant as in the completely classical approach.

\subsection{Implementation Details} 
It is important to note that recursion in this algorithm is only used to control the flow of computation. No matrices are copied or created in the recursive step as all operations happen in-place. 

To select the point where the algorithm switches from recursive to classic multiplication we parametrized the switch point and ran scripted tests to find the optimal block size. This number was different across computers. The computer we used for testing performed best if the switch to classic algorithm happened for blocks smaller than $22$ in any dimension.

\begin{figure}[h]
\centering
\includegraphics[width=0.48\textwidth]{img/recursive.pdf}
\caption{Running-time comparison of recrusive multiplication algorithms: without (\textsf{recursive}) and with (\textsf{recursiveT}) transposition.}
\label{fig:recursive}
\end{figure}



\section{Strassen Algorithm}
\label{section:strassen}

The Strassen algorithm can be derived from the recursive algorithm, as it is only an additional idea on the top of the recursive algorithm.
The idea is to get the block matrices of matrix $C$ with less block products then in the recursive algorithm i.e., less than 8.

So for recursive multiplication $C=AB$ we can define auxiliary matrices as 
\begin{align*}
M_{1} &= (A_{1,1} + A_{2,2})(B_{1,1} + B_{2,2}) \\
M_{2} &= (A_{2,1} + A_{2,2})B_{1,1} \\
M_{3} &= A_{1,1}(B_{1,2} - B_{2,2}) \\
M_{4} &= A_{2,2}(B_{2,1} - B_{1,1}) \\
M_{5} &= (A_{1,1} + A_{1,2})B_{2,2} \\
M_{6} &= (A_{2,1} - A_{1,1})(B_{1,1} + B_{1,2}) \\
M_{7} &= (A_{1,2} - A_{2,2})(B_{2,1} + B_{2,2})
\end{align*}
and compute sub-matrices of C as
\begin{align*}
C_{1,1} &= M_{1} + M_{4} - M_{5} + M_{7} \\
C_{1,2} &= M_{3} + M_{5} \\
C_{2,1} &= M_{2} + M_{4} \\
C_{2,2} &= M_{1} - M_{2} + M_{3} + M_{6}
\end{align*}
%%
Which yields the same result as the recursive algorithm.
In recursive algorithm we need $8$ multiplication of submatrices of $A$ and $B$ but in the Strassen algorithm we need only $7$ multiplications of submatrices of the same size. Additionally, recursive algorithm requires $4$ matrix additions and Strassen requires $12$ matrix additions, and $6$ matrix subtractions. Since multiplication is asymptotically harder than summation or subtraction this drastically improves the theoretical time complexity of the strassen algorithm. Strassen showed that his method has time complexity
$\Theta(n^{\log_{2}(7)})$ or $\textrm{O}(n^{2.8074})$ \cite{Pan1978a}. 

%%Which yields the same result as the recursive algorithm.
%%In recursive algorithm we need $8$ multiplication of submatrices of $A$ and $B$ but in Strassen algorithm we need only $7$ multiplications of submatrices of the same size.Recursive algorithm requires $4$ matrix additions and $8$ matrix multiplications. Strassen requires $12$ matrix additions, $6$ matrix subtractions and $7$ matrix multiplications. TODO(asymptotically time complexity)
%%(TODO mogoce to raje tu This can drastically improve the theoretical time complexity of algorithm.)
%%(TODO Because addition and subtraction requires $O(n^{2})$ time and multiplication with classic algorithm $O(n^{3})$. 

%%Multiplication is asymptotically harder then summation or subtraction. 
%%They can be compared to reading input data but the multiplication is more time consuming)


%%TODO: Strassen time complexitiy: 
%%$O(n^{\log_{2}(7)})$
%% 2.807354922057604107441969317231830808641026625966140783677...
%%$O(2.8074)$
%%(VIR: Matrix Multiplication, Trilinear Decompositions,
%%APA Algorithms, and Summation
%%Victor Y. Pan)

%%\subsection{Subcubic algorithm}
%% TODO kaksno pa bo pimenovanje v datotekah v mapi kjer programiramo???

\subsection{Transposed Strassen-like Algorithm}

Our implementation of a subcubic algorithm can be derived from recursive transposed algorithm similarly to how the Strassen algorithm can be derived from the recursive algorithm. 
Since we did the derivation from recursive transposed algorithm on our own, its blocks are different from the ones in the Strassen algorithm. There exists many Strassen-like algorithms that take the same idea as the Strassen algorithm, but come up with different block organizations. Our goal was to combine the idea of transposing one of the matrices before multiplication with the subcubic nature of Strassen algorithm.

For the transposed recursive multiplication $C = A \times X$ we can define submatrices of $C$ as
\begin{align*}
C_{1,1} &= M_{4} + M_{5}\\
C_{1,2} &= M_{2} - M_{6} + M_{3} - M_{5} \\
C_{2,1} &= M_{1} - M_{4} - M_{3} - M_{7} \\
C_{2,2} &= M_{6} + M_{7}
\end{align*}
where auxiliary matrices $M_{i}$, where $i \leq 7$, are defined as
\begin{align*}
M_{1} &= (A_{1} + B_{1}) \times (X_{1} + Y_{1}) \\
M_{2} &= (A_{2} + B_{2}) \times (X_{2} + Y_{2}) \\
M_{3} &= (A_{1} - B_{2}) \times (X_{2} + Y_{1}) \\
M_{4} &= A_{1} \times (X_{1} - X_{2}) \\
M_{5} &= (A_{2} + A_{1}) \times X_{2} \\
M_{6} &= B_{2} \times (Y_{2} - Y_{1}) \\
M_{7} &= (B_{1} + B_{2}) \times Y_{1}
\end{align*}
In Figure \ref{fig:strassen} we compare our implementations of the Strassen algorithm and its recursive counterpart. As with the recursive algorithm the time gain is not significant. This is because the algorithms switch to the recursive algorithm when block size is lower than 96. As before this threshold was determined experimentally. Since small blocks fit in the cache, memory access patterns do not affect performance much.

% But one should take in to consideration that this is so because the block sizes were also optimized (end block sizes minimal 96 TODO: ce je katerakoli dimenzija bloka manjsa od 96 potem se algoritem preklopi na rekurzivnega).

\begin{figure}[h]
\centering
\includegraphics[width=0.48\textwidth]{img/strassen.pdf}
\caption{Running-time comparison of Strassen multiplication algorithms: without (\textsf{strassen}) and with (\textsf{strassenT}) transposition.}
\label{fig:strassen}
\end{figure}

\section{Numerical Stability}
\label{section:num_stab}

We also compared numerical stability of our implemented algorithms. For this test we changed our algorithms to perform calculations using single-precision floating-point arithmetic (float). We generated two random matrices and multiplied them. We also multiplied the same matrices using double-precision in uBLAS. We then rounded the uBLAS result back to a float and calculated the error estimate as follows
\begin{align*}
\max_{i,j}|(C_{\text{blas}} - C_{\text{test}})_{i,j}|.
\end{align*}
Here we assume that the result computed with double precision is correct for all the digits representable in single precision floating point numbers.

The results in Figure \ref{fig:num_stab} show that the recursive algorithm is the most numerically stable algorithm, followed by classic algorithm and the Strassen algorithm. The transposed Strassen algorithm performs the worst in this test. Although the test shown in Figure \ref{fig:num_stab} shows values only for one particular pair of random matrices, testing with different random matrices produces similar results.

\begin{figure}[h]
\centering
\includegraphics[width=0.48\textwidth]{img/numerical_stability.pdf}
\caption{Maximum absolute error comparing single-precision computation for a $2000 \times 2000$ matrix multiplication with a double-precision computation on the same matrix. }
\label{fig:num_stab}
\end{figure}

%%The Classic algorithm is not perfectly numerically stable because the summation from left to right of $n$ elements in a vector is not perfectly numerically stable.
The Classic algorithm is not perfectly numerically stable, because 
adding arbitrary floating point numbers will usually give some rounding error, which will be proportional to the size of the result. 
%(TODO citat O(n) stabilnost).
We can accomplish better results by applying some smart and expensive numerically stable algorithm like Kahan summation 
\cite{Kahan}
%(TODO citat O(1) stabilnost).
or using a recursive strategy where we first sum pair-wise elements of the vector and continue the procedure until only one element remains.
%(TODO citat O(logn) stabilnost).
%If the beginning elements are of the same magnitude order then also the pairs in this procedure are and so there is not so many digits from one number lost. 
%We should stress that the best practice for summation of the array of numbers is to always sum the numbers of the same size. This can be accomplished by sorting the array, 

The recursive algorithm's way of summation is most similar to recursive summation, however our implementation violates the recursive algorithm scheme by:
\begin{itemize}
\item 
stopping the recursion early and performing the classic multiplication algorithm for sufficiently small blocks and
\item 
adding the values from the small blocks calculated with the classical algorithm directly to the corresponding matrix entries and not first adding the blocks together pair-wise.  
\end{itemize}
This two differences both lower the numerical stability compared to a more strictly implemented recursive algorithm.

Strassen and Strassen-like algorithms are less numerically stable, because if we expand the formula for one matrix entry we see that in the Strassen algorithm the formula is much larger. Some elements are first added and then subtracted, which reduces numerical stability.





% (TODO: premisljen plan dela za numericno stabilnost:
% uporaba float za vsa razlicna mnozenja
% classicT,
% recursiveT,
% strassen,
% strassenMT [mozno ime modified transposed strassen].
% Vse to primerjamo z izracunom iz knjiznice boost z 
% natancnostjo double [pomembno, da se ta poskus izide je, 
% da dejansko na racunalniku kjer se izvaja je razlika med 
% double in float!!!]
% za safe check boosta pa probamo z dvojno natancnostjo tudi ostale metode.
% Poanta pa je vzamemo da je boost z double popolnoma natancen [ceprav v resnici ni] potem pa to primerjamo z nasimi algoritmi, ki uporabljajo float
% se vedno pa bo veljalo, da je boost z dvojno natancnostjo veliko bolj natancen zato...

% Dodatno: 
% Zanimivo je tole: Numerical stability
% %%https://en.wikipedia.org/wiki/Kahan_summation_algorithm 
% tu so predstavljene vse pomembne in zanimive ideje kar se tice sestevanja stevil. Meni je originalni algoritem zelo vsec, 
% ce sem videl pravilno pa je v "izboljsani verziji" napaka, ki lahko povzroci neke numericne napake.

% Je tudi notri ideja o rekurzivnem sestevanju, ki pa sem jo jaz ze imel. Oziroma obstaja se boljsa ideja, ki sem jo imel: 
% vedno zaporedno sestevaj stevila, ki so cimbolj enakega velikostnega ranga, kako to dejansko dobro narediti je drug problem. Nek priblizek je seveda lahko sortiranje stevil po velikosti in nato sestevanje lahko to tudi v kombinaciji z rekurzivnim, nek dober program se v seminarju. 
% Mogoce je to tako zanimivo, da bi tudi objavili zraven v svoji mapi numericna stabilnost...

% Sem veliko razmisljal, teoreticno je to podrocje skoraj nemogoce pokriti. 
% Prakticno, bi za vsako mnozenje lahko naredil primere, da daluje najbolje oziroma najslabse, to velja omeniti, bi pa dejanska izvedba mocno zmotila jasnost clanka zato TEGA NE BI DAL NOTRI.
% Se mi zdi, da bo se najboljsi prakticni preizkus za nakljucna stevila med -1 in 1. Se mi zdi, da je primerno da damo tudi negativna. Okoli 0 je tudi najvecja natancnost double stevil, torej ta standard je najbolj natancen za stevia blizu 0...

% No in pojavi se vprasanje s cim primerjati dobljen rezultat ce ga v bistvu ne ves. Zato gremo en korak nazaj na float in predpostavimo, da je rezultat z double bolj natancen. Ostale algoritme nato primerjamo s tem rezultatom.

% Zdi se mi smiselno, da je velikost matrik 2000 v obe dimenziji. Neke rezultate bomo dobili, prevec pa se s tem ni za obremenjevati,.. 
% Bolj podamo da smo iskali ce je na tem podrocju kaj drasticnega ugotovitev pa bo da ni

% Pa se to pravilnosti ne bomo prevejali samo za en element ampak po neki normi. Najbolje kar maksimalni, ki pogleda kateri element v matriki razlike najbolj odstopa od 0. Torej gleda kriticni element.

% DODATNO TODO; sem se spomnil, da nekje v clanku lepo omeniti, da se za casovno zahtevnost vedno splaca, da ima mnozenje z vektorjem predost pred mnozenjem matrik, ce hocemo hitro delovanje. Treba je se premisliti, kam to umestiti.
% )

% First thing we noticed was that numerical stabilitiy is not a major problem for matrix multiplication problem.
% (TODO tole bom pa jaz se preveril na zelo velikih matrikah, 
% vseeno je res, da ko vecamo velikost matrik se natacnost strassna za en element manjsa, seveda se prav tako manjsa tudi natancnost klasicnega mnozenja matrik, rekurzivno mnozeje v splosnem trpi se najmanj posledic pri tem problemu. Vseeno pa jih. Zato je res celo naslednje, za vsako metodo obstaja razred matrik, ki jih bo ta metoda izracunala bolje od ostalih [saj se lahko v najboljsem primeru numericne napake celo odstejejo], velja pa poudariti, da je seveda rekurzivno mnozenje najboljse za najvecje stevilo matrik. 
% Mogoce bi bilo tu celo najbolj korektno, da za vsako mnozenje podamo nek tip matrik, za katerega je to mnozenje najbolj ucinkovito, 
% ja tako res pokazemo, da je to lahko povsod problem, in mogoce se utemeljitev zakaj je za matrike z nakljucnimi elementi med 0 in 1 rekurzivno mnozenje najboljse.
% Ampak to so matematicno zelo zahtevne teme.)

% (TODO na tem mestu mogoce vseeno vsaj nekaj komentarja o numericni stabilnosti, mi smo namrec algoritem testirali za realna stevila zato je to vprasanje relavantno, ce bi testirali za int to ne bi bilo tako pomembno, bi pa tam pri nekaterih algoritmih lahko prislo do overflow-a, ki pa ne bi bil problematicen ce racunamo po modulu)

%%TODO
%%TODO to je treba se dobro premisliti kako lepo napisati, celo to poglavje
%%Numerical stability of this kind of matrix multiplication can be compared to numerical stability of scalar product of 2 vectors. 
%%It can be improved if we always sum the two numbers that are of roughly same absolute size. 
%% TODO jaz bi to veliko raje povedal abstraknto in ne s konkretnim primerom ampak ne vem kako
%% TODO ta primer kvari strokovnost clanka, se mi pa zdi da pa ta ideja mora biti notri saj je zanimiva in netrivialna
%%So for the summation of positive numbers let us say $x_{1},x_{2},x_{3},x_{4}$ which are of equal size class it is better to use the formula $(x_{1}+x_{2})+(x_{3}+x_{4})$ than formula $((x_{1}+x_{2})+x_{3})+x_{4}$.
%%Numerical stability of classic matrix multiplication and transposed matrix multiplication is the same because the formula for computing one element is the same.
% \begin{align*}
% c_{i,j} = \sum_{k=1}^{n} a_{i,k} \widetilde{b}_{j,k}
% \end{align*}
% IMPORTATNT NOTICE:
% $((x_{1}+x_{2})+x_{3})+x_{4}$ is not equal to $(x_{1}+x_{2})+(x_{3}+x_{4})$ 
% for the positive number of the same order the second is more numerically stable.

\section{Experimental Comparison}
\label{section:experim}

\subsection{Implementation}
We implemented the algorithms in C\texttt{++}. We tried to optimize each algorithm as much as possible. Although all our comparisons were performed on square matrices we also tested correctness on non-square matrices. Implementation of our algorithms is available on Github at \texttt{https://github.com/mihic/matrixmul}.

\subsection{uBLAS Library}
In order to compare our implementations with an existing state-of-the-art implementations we chose uBLAS, the implementation of matrix multiplication from the Boost library in the \cpp programming language. We chose it for its ease of use and simplicity. Other libraries such as Eigen \cite{eigen} or Intel Math Kernel Library \cite{mkl} are vigorously optimized in order to be able to use multiple cores and advanced vector instructions of modern processors. We chose uBLAS since our goal was to compare generic optimization techniques, not specific to particular processor architecture.

We used the \texttt{block\_prod} function from \texttt{boost::numeric::ublas} with a block size of $32$. As before we determined the optimal block size experimentally.

%%TODO zastavice:
%% -DNDEBUG
%% -DBOOST UBLAS NDEBUG

\subsection{Testing Environment}
All our tests were run on a computer with 16 GBs of RAM on Intel's Core i7-6700HQ locked to 3.1GHz. The operating system was Arch Linux using kernel version 4.13. We used \textit{gcc} version \texttt{7.2.0}, with the following compilation flags: \texttt{-O3 -march=native -DNDEBUG -DBOOST\_UBLAS\_NDEBUG}. \texttt{O3} and \texttt{march=native} flags enable compiler optimizations. The other two flags disable checking for errors inside the uBLAS library. Without those flags the uBLAS library checks the numbers in the calculation and warns the user if it finds that the computation is loosing too much precision. The flags disable this overhead and significantly improve the performance of uBLAS. 

 All the compared algorithms used a single thread of execution and calculated a product of two $n \times n$ randomly generated matrices. 

%%TODO: govor o vplivu cache na vse skupi

\subsection{Results}

\begin{figure}[h]
\centering
\includegraphics[width=0.48\textwidth]{img/all_5000.pdf}
\caption{Running-time comparison of the faster version of discussed algorithms, including the uBLAS library for comparison.}
\label{fig:all_algorithms}
\end{figure}

In Figure \ref{fig:all_algorithms} we compare the faster versions of all described algorithms. As expected transposed classic algorithm is slowest, followed by transposed recursive and uBLAS. Since uBLAS uses a block algorithm we can confirm our prediction, that recursive algorithm should behave similarly to a block algorithm. The best algorithm in this comparison is modified strassen. 
%%(TODO: Brez expected ker sva preklop 96 nastimala tako da sva vedela da bo bolje; to se lahko tudi omeni? Pisalo bi nekaj takega: These results show that modified strassen is notably [?? a je to ok] faster even for smaller $n$.)
%% To vejetno ostane tako kot je bolje se tezko pove
Although this was expected for large $n$, these results show that the transposed Strassen algorithm is faster even for smaller $n$.
%% TODO a je to res pravilno da se tako napise? Kaj je misljeno z little downsides? Pa se to midva sva tako nastimala z preklopom algoritmov da bo Strassen nujno hitrejsi. Ko pri neki konstanti menja na rekurzivnega. Nekaj o tem morava napisati, kje in kako pa je treba razmisliti?
%% Mogoce bi tu vseeno dala se en enak graf le do 1000?? Lahko se tudi z besedami stevilcno navede nekaj rezultatov (ki se jih sicer da razbrati z grafa)
%% Sem pogledal v program konstanta za preklop iz strassna na rekurzivnega je 96
%% Pogoj izgleda takole:
% if(std::min({aD, bD, cD})<96){
%         rek_tra_mno_pomozna(mat1, mat4, mat3, a1, a2,  b1, b2, c1, c2);
% }

\section{Conclusion}
We described a few practical matrix multiplication methods and discussed their implementation details, numerical stability and performance. We can conclude that the Strassen-like algorithm does not have a significant constant hindering its performance for practically sized matrices. This would suggest that this method could be implemented even in general purpose libraries with the only downside being lower numerical stability. The library could provide a fast Strassen based algorithm and a fall-back recursive algorithm for cases when numerical stability is very important. 

Our work could be extended to include processor specific optimizations such as SIMD (Single Instruction Multiple Data) instructions. This would allow us to compare the Strassen algorithm with highly optimized libraries such as Eigen or Intel MKL. 
\balance


%% (TODO to je sicer malo grdo ampak jaz bi dal celo nek marketinski stavek v takem smislu: for matrices of size larger than 5000 we can see that the use of the strassen method gives us 100 \%  (percent) faster time performance comperd to simple multiplication methods)
\section*{Acknowledgements}
The authors would like to thank Jurij Mihelič, because our work began as an assignment during his MSc course \textit{Algorithm Engineering} and he encouraged and helped us expanding it into an article.

\bibliographystyle{plain}
\bibliography{viri}




\end{document}


